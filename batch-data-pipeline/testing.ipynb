{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "\n",
    "import snowflake.connector\n",
    "from confluent_kafka import Consumer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "KAFKA_SERVER = os.getenv('KAFKA_SERVER')\n",
    "KAFKA_USERNAME=os.getenv('KAFKA_USERNAME')\n",
    "KAFKA_PASSWORD=os.getenv('KAFKA_PASSWORD')\n",
    "KAFKA_TOPIC_NAME = os.getenv('KAFKA_TOPIC_NAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subscribe_to_kafka_topic():\n",
    "    \"\"\"Produce a consumer that subscribes to the relevant Kafka topic\"\"\"\n",
    "    c = Consumer({\n",
    "    'bootstrap.servers': KAFKA_SERVER,\n",
    "    'group.id': f'deleton' + str(uuid.uuid1()),\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanisms': 'PLAIN',\n",
    "    'sasl.username': KAFKA_USERNAME,\n",
    "    'sasl.password': KAFKA_PASSWORD,\n",
    "    'session.timeout.ms': 6000,\n",
    "    'heartbeat.interval.ms': 1000,\n",
    "    'fetch.wait.max.ms': 6000,\n",
    "    'auto.offset.reset': 'latest',\n",
    "    'enable.auto.commit': 'false',\n",
    "    'max.poll.interval.ms': '86400000',\n",
    "    'topic.metadata.refresh.interval.ms': \"-1\",\n",
    "    \"client.id\": 'id-002-005',\n",
    "})\n",
    "\n",
    "    c.subscribe([KAFKA_TOPIC_NAME])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values_from_log(string):\n",
    "    \"\"\"Extract numerical values from Kafka log using regular expression\"\"\"\n",
    "    regexp = r'\\d+.?\\d+|\\d'\n",
    "    numerical_values = re.findall(regexp,string)\n",
    "    return numerical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_system_log(c):\n",
    "    \"\"\"Waits for current user when starting script to finish their ride, as their user data is not retrievable\"\"\"\n",
    "    print('Waiting for first user to finish... This may take some time.')\n",
    "    while True:\n",
    "        kafka_message = c.poll(0.5)\n",
    "\n",
    "        if kafka_message is not None:\n",
    "            kafka_log = kafka_message.value().decode('utf-8')\n",
    "\n",
    "            if 'SYSTEM' in kafka_log:\n",
    "                print('First user has finished, now beginning data processing')\n",
    "                return kafka_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_to_date(unix_timestamp):\n",
    "    \"\"\"Converts unix timestamp to datetime\"\"\"\n",
    "    unix_timestamp /= 1000  #convert to seconds as unix timestamp is in milliseconds\n",
    "    converted_to_date = datetime.utcfromtimestamp(unix_timestamp).strftime('%Y-%m-%d')\n",
    " \n",
    "    return converted_to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_full_name(name):\n",
    "    \"\"\"Split full name based on various conditions, such as if they have a title or not, checks to see if they have a last name\"\"\"\n",
    "    name_list = name.split(' ')\n",
    "    titles = ['Mr','Mrs','Miss','Ms','Dr']\n",
    "\n",
    "    if name_list[0] in titles: #Exclude user titles from snowflake\n",
    "        first_name = name_list[1]\n",
    "\n",
    "        try:\n",
    "            last_name = name_list[2]\n",
    "\n",
    "        except IndexError: #Catch index error incase user did not give lastname\n",
    "            last_name = None\n",
    "\n",
    "    else:\n",
    "        first_name = name_list[0]\n",
    "        last_name = name_list[1]\n",
    "\n",
    "    return first_name,last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(address_list):\n",
    "    \"\"\"Expands sublists present in lists, which is a consequence of splitting addresses\"\"\"\n",
    "    flat_list = []\n",
    "\n",
    "    for element in address_list:\n",
    "        if type(element) is list:\n",
    "          \n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def split_address(address):\n",
    "    \"\"\"Splits address based on various conditions, such as if they have whitespace or commas, or how many elements they have\"\"\"\n",
    "    address_list = address.split(',')\n",
    "\n",
    "    if len(address_list) < 4:\n",
    "        address_list[0] = address_list[0].split(' ', 1) \n",
    "        address_list = flatten_list(address_list)\n",
    "\n",
    "    house_number = address_list[0]\n",
    "    street_name = address_list[1].title()\n",
    "    region = address_list[2].title()\n",
    "    postcode = address_list[3]\n",
    "\n",
    "    return house_number,street_name,region,postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_user_data(user_dictionary):\n",
    "    \"\"\"Clean user data by converting timestamps, obtaining first and lastname, and splitting address field\"\"\"\n",
    "    user_dictionary['account_create_date'] = convert_unix_to_date(user_dictionary['account_create_date'])\n",
    "    user_dictionary['date_of_birth'] = convert_unix_to_date(user_dictionary['date_of_birth'])\n",
    "\n",
    "    first_name,last_name = split_full_name(user_dictionary['name'])\n",
    "\n",
    "    user_dictionary['first_name'] = first_name\n",
    "    user_dictionary['last_name'] = last_name\n",
    "\n",
    "    house_number,street_name,region,postcode = split_address(user_dictionary['address'])\n",
    "\n",
    "    user_dictionary['house_number'] = house_number\n",
    "    user_dictionary['street_name'] = street_name\n",
    "    user_dictionary['region'] = region\n",
    "    user_dictionary['postcode'] = postcode\n",
    "    user_dictionary['gender'] = user_dictionary['gender'].title()\n",
    "\n",
    "    return user_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assets.engine_wrapper import database_connection\n",
    "from sqlalchemy import create_engine,inspect\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT=os.getenv('DB_PORT')\n",
    "DB_USER=os.getenv('DB_USER')\n",
    "DB_PASSWORD=os.getenv('DB_PASSWORD')\n",
    "DB_NAME=os.getenv('DB_NAME')\n",
    "\n",
    "\n",
    "conn = database_connection(DB_NAME,DB_USER,DB_PASSWORD,DB_HOST,DB_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_system_log(log):\n",
    "    \"\"\"Obtains user dictionary from system log\"\"\"\n",
    "    system_log = json.loads(log).get('log')\n",
    "    split_log = system_log.split(' mendoza v9: [SYSTEM] data = ')\n",
    "    begin_timestamp = split_log[0][:-7] #remove milliseconds from timestamp for snowflake compatability\n",
    "    \n",
    "    dictionary_string = split_log[1][:-1]\n",
    "    user_dictionary = json.loads(dictionary_string)\n",
    "    \n",
    "    user_dictionary = clean_user_data(user_dictionary)\n",
    "    return begin_timestamp, user_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_users(cs,user_dictionary):\n",
    "    \"\"\"Makes insert query into users table once all relevant information has been obtained\"\"\"\n",
    "    cs.execute(\n",
    "                \"\"\"INSERT INTO users(user_id, first_name, last_name, gender, date_of_birth, \n",
    "                height_cm, weight_kg, house_name, street, region, postcode, email, account_created) \"\"\"\n",
    "                \"VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\",\n",
    "                (\n",
    "                user_dictionary['user_id'],user_dictionary['first_name'],user_dictionary['last_name'],user_dictionary['gender'],\n",
    "                user_dictionary['date_of_birth'],user_dictionary['height_cm'],user_dictionary['weight_kg'],user_dictionary['house_number'],\n",
    "                user_dictionary['street_name'],user_dictionary['region'],user_dictionary['postcode'],\n",
    "                user_dictionary['email_address'],user_dictionary['account_create_date']\n",
    "                )\n",
    "                )\n",
    "    print('made insert into users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_rides(cs, user_dictionary, begin_timestamp, duration, total_power,\n",
    "                mean_power, mean_resistance, mean_rpm, mean_heart_rate):\n",
    "    \"\"\"Makes insert query into rides table once all relevant information has been obtained\"\"\"\n",
    "    cs.execute(\n",
    "                \"INSERT INTO rides(user_id, begin_timestamp, total_duration_sec, total_power, mean_power, mean_resistance, mean_rpm, mean_heart_rate) \"\n",
    "                \"VALUES(%s,%s,%s,%s,%s,%s,%s,%s)\",\n",
    "                (user_dictionary['user_id'], begin_timestamp, duration, \n",
    "                total_power, mean_power, mean_resistance, \n",
    "                mean_rpm, mean_heart_rate)\n",
    "                \n",
    "                )\n",
    "    print('made insert into rides')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for first user to finish... This may take some time.\n",
      "First user has finished, now beginning data processing\n",
      "made insert queries\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m                 heart_rate_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                 resistance_list \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m polling_kafka(c)\n",
      "\u001b[1;32m/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb Cell 13\u001b[0m in \u001b[0;36mpolling_kafka\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m rpm_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     kafka_message \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39;49mpoll(\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m wait_for_first_user:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hakanbas/Documents/group-project/Deloton-Exercise-Bikes/batch-data-pipeline/testing.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         kafka_message \u001b[39m=\u001b[39m wait_for_system_log(c)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c = subscribe_to_kafka_topic()\n",
    "\n",
    "def polling_kafka(c):\n",
    "    \"\"\"Polls kafka in an infinite loop, making insert queries into snowflake tables once data processing has finished\"\"\"\n",
    "    wait_for_first_user = True #Wait for current user to finish, so can obtain all relevant information\n",
    "    first_user_collected = False #Add check for first user so data is not sent to snowflake prematurely\n",
    "\n",
    "    resistance_list = []\n",
    "    power_list = []\n",
    "    heart_rate_list = []\n",
    "    rpm_list = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        kafka_message = c.poll(0.5)\n",
    "\n",
    "        if wait_for_first_user:\n",
    "            kafka_message = wait_for_system_log(c)\n",
    "            wait_for_first_user = False\n",
    "\n",
    "\n",
    "        if kafka_message is not None: #exclude none values\n",
    "            log = kafka_message.value().decode('utf-8')\n",
    "\n",
    "            if 'SYSTEM' in log:\n",
    "                first_user_collected = True\n",
    "                begin_timestamp,user_dictionary = dict_from_system_log(log)\n",
    "                \n",
    "\n",
    "            elif 'INFO' in log: #only check for strings with INFO\n",
    "\n",
    "                values = json.loads(log)\n",
    "                log = values.get('log')\n",
    "            \n",
    "\n",
    "                if 'Ride' in log: #process strings with Ride info\n",
    "                    split_by_timestamp_and_logs = ' mendoza v9: [INFO]: Ride - '\n",
    "                    timestamp_and_values = log.split(split_by_timestamp_and_logs)\n",
    "                \n",
    "                    log_values = extract_values_from_log(timestamp_and_values[1])\n",
    "\n",
    "                    duration = int(float(log_values[0]))\n",
    "                    resistance_list.append(int(log_values[1]))\n",
    "            \n",
    "\n",
    "                elif 'Telemetry' in log:\n",
    "                    split_by_timestamp_and_logs = ' mendoza v9: [INFO]: Telemetry - '\n",
    "                    timestamp_and_values = log.split(split_by_timestamp_and_logs)\n",
    "                \n",
    "                    log_values = extract_values_from_log(timestamp_and_values[1])\n",
    "\n",
    "                    heart_rate_list.append(int(log_values[0]))\n",
    "                    rpm_list.append(int(log_values[1]))\n",
    "                    power_list.append(round(float(log_values[2]),3))\n",
    "\n",
    "\n",
    "            elif 'new ride' in log and first_user_collected: #New user is starting, so load collected data into snowflake and reset\n",
    "                total_power = sum(power_list)\n",
    "                mean_power = mean(power_list)\n",
    "                mean_rpm = mean(rpm_list)\n",
    "                mean_heart_rate = mean(heart_rate_list)\n",
    "                mean_resistance = mean(resistance_list)\n",
    "                \n",
    "                conn.insert_users_query(user_dictionary)\n",
    "                conn.insert_rides_query(user_dictionary,begin_timestamp,duration,total_power,mean_power,mean_resistance,mean_rpm,mean_heart_rate)\n",
    "                print('made insert queries')\n",
    "                \n",
    "                power_list = []\n",
    "                rpm_list = []\n",
    "                heart_rate_list = []\n",
    "                resistance_list = []\n",
    "                \n",
    "polling_kafka(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4649, datetime.datetime(2022, 10, 7, 13, 18, 15), 518, 21648.178000000087, 41.791849420849424, 40.826254826254825, 40.00772200772201, 91.23166023166023), (2, 4649, datetime.datetime(2022, 10, 7, 13, 44, 21), 518, 27764.09499999998, 53.598638996138995, 41.027027027027025, 42.673745173745175, 118.93822393822394)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1665151119.429|SESSTMOUT|id-002-005#consumer-4| [thrd:main]: Consumer group session timed out (in join-state steady) after 81192 ms without a successful response from the group coordinator (broker 9, last error was Success): revoking assignment and rejoining group\n",
      "%5|1665151119.561|REQTMOUT|id-002-005#consumer-4| [thrd:sasl_ssl://b13-pkc-l6wr6.europe-west2.gcp.confluent.cloud:9092/]: sasl_ssl://b13-pkc-l6wr6.europe-west2.gcp.confluent.cloud:9092/13: Timed out FetchRequest in flight (after 81474ms, timeout #0)\n",
      "%4|1665151119.561|REQTMOUT|id-002-005#consumer-4| [thrd:sasl_ssl://b13-pkc-l6wr6.europe-west2.gcp.confluent.cloud:9092/]: sasl_ssl://b13-pkc-l6wr6.europe-west2.gcp.confluent.cloud:9092/13: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1665151119.561|FAIL|id-002-005#consumer-4| [thrd:sasl_ssl://b13-pkc-l6wr6.europe-west2.gcp.confluent.cloud:9092/]: sasl_ssl://b13-pkc-l6wr6.europe-west2.gcp.confluent.cloud:9092/13: 1 request(s) timed out: disconnect (after 2876707ms in state UP)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "with engine.connect() as connection:\n",
    "    query = \"\"\"SELECT * FROM zookeepers_production.rides\"\"\"\n",
    "    result = connection.execute(query)\n",
    "print(result.fetchall())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
